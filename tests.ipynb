{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\fux-issues-bot\\venv\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in f:\\fux-issues-bot\\venv\\lib\\site-packages (1.19.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: requests in f:\\fux-issues-bot\\venv\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests) (2020.6.20)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: spacy in f:\\fux-issues-bot\\venv\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (1.19.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (1.10.13)\n",
      "Requirement already satisfied: jinja2 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (2.11.2)\n",
      "Requirement already satisfied: setuptools in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (65.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from spacy) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
      "Requirement already satisfied: colorama in f:\\fux-issues-bot\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: trafilatura in f:\\fux-issues-bot\\venv\\lib\\site-packages (1.6.2)\n",
      "Requirement already satisfied: certifi in f:\\fux-issues-bot\\venv\\lib\\site-packages (from trafilatura) (2020.6.20)\n",
      "Requirement already satisfied: courlan>=0.9.4 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from trafilatura) (0.9.4)\n",
      "Requirement already satisfied: htmldate>=1.5.1 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from trafilatura) (1.5.1)\n",
      "Requirement already satisfied: justext>=3.0.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from trafilatura) (3.0.0)\n",
      "Requirement already satisfied: lxml>=4.9.3 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from trafilatura) (4.9.3)\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from trafilatura) (3.3.0)\n",
      "Collecting urllib3<3,>=1.26 (from trafilatura)\n",
      "  Obtaining dependency information for urllib3<3,>=1.26 from https://files.pythonhosted.org/packages/37/dc/399e63f5d1d96bb643404ee830657f4dfcf8503f5ba8fa3c6d465d0c57fe/urllib3-2.0.5-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.0.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: langcodes>=3.3.0 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from courlan>=0.9.4->trafilatura) (3.3.0)\n",
      "Requirement already satisfied: tld>=0.13 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from courlan>=0.9.4->trafilatura) (0.13)\n",
      "Requirement already satisfied: dateparser>=1.1.2 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from htmldate>=1.5.1->trafilatura) (1.1.8)\n",
      "Collecting python-dateutil>=2.8.2 (from htmldate>=1.5.1->trafilatura)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     -------------------------------------- 247.7/247.7 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz in f:\\fux-issues-bot\\venv\\lib\\site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (2020.1)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (2020.7.14)\n",
      "Requirement already satisfied: tzlocal in f:\\fux-issues-bot\\venv\\lib\\site-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (5.0.1)\n",
      "Requirement already satisfied: six>=1.5 in f:\\fux-issues-bot\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->htmldate>=1.5.1->trafilatura) (1.15.0)\n",
      "Requirement already satisfied: tzdata in f:\\fux-issues-bot\\venv\\lib\\site-packages (from tzlocal->dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (2023.3)\n",
      "Requirement already satisfied: backports.zoneinfo in f:\\fux-issues-bot\\venv\\lib\\site-packages (from tzlocal->dateparser>=1.1.2->htmldate>=1.5.1->trafilatura) (0.2.1)\n",
      "Downloading urllib3-2.0.5-py3-none-any.whl (123 kB)\n",
      "   ---------------------------------------- 123.8/123.8 kB 2.4 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, python-dateutil\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.10\n",
      "    Uninstalling urllib3-1.25.10:\n",
      "      Successfully uninstalled urllib3-1.25.10\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.1\n",
      "    Uninstalling python-dateutil-2.8.1:\n",
      "      Successfully uninstalled python-dateutil-2.8.1\n",
      "Successfully installed python-dateutil-2.8.2 urllib3-2.0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nbconvert 7.6.0 requires jinja2>=3.0, but you have jinja2 2.11.2 which is incompatible.\n",
      "nbconvert 7.6.0 requires markupsafe>=2.0, but you have markupsafe 1.1.1 which is incompatible.\n",
      "requests 2.24.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 2.0.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install beautifulsoup4\n",
    "!pip install numpy\n",
    "!pip install requests\n",
    "!pip install spacy\n",
    "!pip install trafilatura\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:08:04.569721200Z",
     "start_time": "2023-09-30T19:07:45.660081700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests.models import MissingSchema\n",
    "import spacy\n",
    "import trafilatura\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:29:09.438407500Z",
     "start_time": "2023-09-30T19:29:06.947897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "urls = ['https://website.understandingdata.com/',\n",
    "        'https://sempioneer.com/', ]\n",
    "data = {}\n",
    "\n",
    "for url in urls:\n",
    "    # 1. Obtain the response:\n",
    "    resp = requests.get(url)\n",
    "\n",
    "    # 2. If the response content is 200 - Status Ok, Save The HTML Content:\n",
    "    if resp.status_code == 200:\n",
    "        data[url] = resp.text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:29:20.626130500Z",
     "start_time": "2023-09-30T19:29:19.277959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Главное управление специальных программ Президента Российской Федерации (ГУСП) является федеральным органом исполнительной власти (федеральным агентством), осуществляющим в пределах своей компетенции функции по обеспечению исполнения Президентом Российской Федерации полномочий в сфере мобилизационной подготовки и мобилизации в Российской Федерации.\n",
      "Полное наименование: Главное управление специальных программ Президента Российской Федерации\n",
      "Сокращенное наименование: ГУСП\n",
      "Датой образования Главного управления специальных программ Президента Российской Федерации (cокращенное наименование - ГУСП) является 6 января 1977 года, когда постановлением Совмина РСФСР было образовано Пятое управление Управления делами Совмина РСФСР. ГУСП является его преемником.\n",
      "Основными задачами, возложенными на Пятое управление, были функции заказчика по строительству новых, модернизации и реконструкции существующих пунктов управления Совета Министров РСФСР, поддержанию их в постоянной готовности к применению по прямому предназначению, а также координация деятельности министерств и ведомств по поддержанию в готовности существующих и строительству новых запасных пунктов управления Российской Федерации, а также методическое руководство по вопросам поддержания в готовности запасных пунктов управления субъектов РСФСР. Кроме того, с 1991 года на Пятое управление были возложены функциональные обязанности организации и обеспечения мобилизационной подготовки Управления делами Совмина РСФСР, а с января 1992 года - Администрации Президента и Аппарата Правительства Российской Федерации.\n",
      "Указом Президента Российской Федерации от 5 августа 1991 г. № 32 Пятое управление было введено в состав Администрации Президента РСФСР, а с 24 сентября 1992 г. Распоряжением Президента РСФСР – преобразовано в Управление планирования и реализации специальных программ Администрации Президента Российской Федерации.\n",
      "Указом Президента Российской Федерации от 5 января 1994 г. Управление планирования и реализации специальных программ Администрации Президента Российской Федерации было преобразовано в Главное управление специальных программ Президента Российской Федерации. Был создан государственный орган, объединивший не только подразделения, обеспечивающие подготовку документальной мобилизационной базы, но и систему специальных объектов. Главное управление в качестве структурного подразделения вошло в состав Администрации Президента Российской Федерации.\n",
      "В соответствии с Указом Президента Российской Федерации от 30 апреля 1998 г. № 483 «О структуре федеральных органов исполнительной власти» Главное управление было отнесено к федеральным органам исполнительной власти, руководство деятельностью которых осуществляет Президент Российской Федерации.\n",
      "В соответствии со статьей 32 Федерального конституционного закона от 17 декабря 1997 г. № 2-ФКЗ «О Правительстве Российской Федерации» и Указом Президента Российской Федерации от 31 декабря 2017 г. № 651 «Вопросы Главного управления специальных программ Президента Российской Федерации» утверждено Положение о Главном управлении.\n",
      "Сегодня Главное управление является федеральным органом исполнительной власти (федеральным агентством), осуществляющим в пределах своей компетенции функции по обеспечению исполнения Президентом Российской Федерации полномочий в сфере мобилизационной подготовки и мобилизации в Российской Федерации в соответствии с Федеральным законом от 26 февраля 1997 г. № 31-ФЗ «О мобилизационной подготовке и мобилизации в Российской Федерации».\n"
     ]
    }
   ],
   "source": [
    "def beautifulsoup_extract_text_fallback(response_content):\n",
    "    '''\n",
    "    This is a fallback function, so that we can always return a value for text content.\n",
    "    Even for when both Trafilatura and BeautifulSoup are unable to extract the text from a\n",
    "    single URL.\n",
    "    '''\n",
    "\n",
    "    # Create the beautifulsoup object:\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "\n",
    "    # Finding the text:\n",
    "    text = soup.find_all(text=True)\n",
    "\n",
    "    # Remove unwanted tag elements:\n",
    "    cleaned_text = ''\n",
    "    blacklist = [\n",
    "        '[document]',\n",
    "        'noscript',\n",
    "        'header',\n",
    "        'html',\n",
    "        'meta',\n",
    "        'head',\n",
    "        'input',\n",
    "        'script',\n",
    "        'style', ]\n",
    "\n",
    "    # Then we will loop over every item in the extract text and make sure that the beautifulsoup4 tag\n",
    "    # is NOT in the blacklist\n",
    "    for item in text:\n",
    "        if item.parent.name not in blacklist:\n",
    "            cleaned_text += '{} '.format(item)\n",
    "\n",
    "    # Remove any tab separation and strip the text:\n",
    "    cleaned_text = cleaned_text.replace('\\t', '')\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def extract_text_from_single_web_page(url):\n",
    "    downloaded_url = trafilatura.fetch_url(url)\n",
    "    try:\n",
    "        a = trafilatura.extract(downloaded_url, output_format='json', with_metadata=True, include_comments=False,\n",
    "                                date_extraction_params={'extensive_search': True, 'original_date': True})\n",
    "    except AttributeError:\n",
    "        a = trafilatura.extract(downloaded_url, output_format='json', with_metadata=True,\n",
    "                                date_extraction_params={'extensive_search': True, 'original_date': True})\n",
    "    if a:\n",
    "        json_output = json.loads(a)\n",
    "        return json_output['text']\n",
    "    else:\n",
    "        try:\n",
    "            resp = requests.get(url)\n",
    "            # We will only extract the text from successful requests:\n",
    "            if resp.status_code == 200:\n",
    "                return beautifulsoup_extract_text_fallback(resp.content)\n",
    "            else:\n",
    "                # This line will handle for any failures in both the Trafilature and BeautifulSoup4 functions:\n",
    "                return np.nan\n",
    "        # Handling for any URLs that don't have the correct protocol\n",
    "        except MissingSchema:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "single_url = 'http://www.fsb.ru/'\n",
    "text = extract_text_from_single_web_page(url=single_url)\n",
    "print(text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T19:52:21.186462400Z",
     "start_time": "2023-09-30T19:52:20.691475700Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
