{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install spacy\n",
    "!pip install trafilatura\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import urllib\n",
    "from requests.models import MissingSchema\n",
    "import spacy\n",
    "import trafilatura\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  #%%\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T14:56:51.790892500Z",
     "start_time": "2023-10-01T14:56:51.753904800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "urls = ['https://website.understandingdata.com/',\n",
    "        'https://sempioneer.com/', ]\n",
    "data = {}\n",
    "\n",
    "for url2 in urls:\n",
    "    # 1. Obtain the response:\n",
    "    resp = requests.get(url2)\n",
    "\n",
    "    # 2. If the response content is 200 - Status Ok, Save The HTML Content:\n",
    "    if resp.status_code == 200:\n",
    "        data[url2] = resp.text\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T14:51:00.800560100Z",
     "start_time": "2023-10-01T14:50:59.763580200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse_page: http://www.fsb.ru/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32mc:\\Windows\\Temp\\ipykernel_16916\\3194121874.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    101\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[0msingle_url\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'http://www.fsb.ru/'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 103\u001B[1;33m \u001B[0mpage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtext\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mparse_page\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msingle_url\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    104\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0murls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Windows\\Temp\\ipykernel_16916\\3194121874.py\u001B[0m in \u001B[0;36mparse_page\u001B[1;34m(link)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0ma\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[0mjson_output\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ma\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m         \u001B[0mprocess_page\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdownloaded_url\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mdownloaded_url\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mjson_output\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'text'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\Windows\\Temp\\ipykernel_16916\\3194121874.py\u001B[0m in \u001B[0;36mprocess_page\u001B[1;34m(response)\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mprocess_page\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 69\u001B[1;33m     \u001B[0mall_relative\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mre\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfindall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'href=(\\'|\")(.*?)(\\'|\")'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     70\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0maa\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_relative\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     71\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m'http'\u001B[0m \u001B[1;32min\u001B[0m \u001B[0maa\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "def beautifulsoup_extract_text_fallback(response_content):\n",
    "    '''\n",
    "    This is a fallback function, so that we can always return a value for text content.\n",
    "    Even for when both Trafilatura and BeautifulSoup are unable to extract the text from a\n",
    "    single URL.\n",
    "    '''\n",
    "\n",
    "    # Create the beautifulsoup object:\n",
    "    soup = BeautifulSoup(response_content, 'html.parser')\n",
    "\n",
    "    # Finding the text:\n",
    "    text = soup.find_all(text=True)\n",
    "\n",
    "    # Remove unwanted tag elements:\n",
    "    cleaned_text = ''\n",
    "    blacklist = [\n",
    "        '[document]',\n",
    "        'noscript',\n",
    "        'header',\n",
    "        'html',\n",
    "        'meta',\n",
    "        'head',\n",
    "        'input',\n",
    "        'script',\n",
    "        'style', ]\n",
    "\n",
    "    # Then we will loop over every item in the extract text and make sure that the beautifulsoup4 tag\n",
    "    # is NOT in the blacklist\n",
    "    for item in text:\n",
    "        if item.parent.name not in blacklist:\n",
    "            cleaned_text += '{} '.format(item)\n",
    "\n",
    "    # Remove any tab separation and strip the text:\n",
    "    cleaned_text = cleaned_text.replace('\\t', '')\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def parse_page(link):\n",
    "    print(f'parse_page: {link}')\n",
    "    downloaded_url = trafilatura.fetch_url(link, no_ssl=True)\n",
    "    try:\n",
    "        a = trafilatura.extract(downloaded_url, output_format='json', with_metadata=False, include_comments=False,\n",
    "                                date_extraction_params={'extensive_search': True, 'original_date': True})\n",
    "    except AttributeError:\n",
    "        a = trafilatura.extract(downloaded_url, output_format='json', with_metadata=False,\n",
    "                                date_extraction_params={'extensive_search': True, 'original_date': True})\n",
    "    if a:\n",
    "        json_output = json.loads(a)\n",
    "        process_page(downloaded_url)\n",
    "        return downloaded_url, json_output['text']\n",
    "    else:\n",
    "        try:\n",
    "            r = requests.get(link)\n",
    "            # We will only extract the text from successful requests:\n",
    "            if r.status_code == 200:\n",
    "\n",
    "                process_page(r)\n",
    "\n",
    "                return r.content.decode('utf-8'), beautifulsoup_extract_text_fallback(r.content)\n",
    "            else:\n",
    "                # This line will handle for any failures in both the Trafilature and BeautifulSoup4 functions:\n",
    "                return \"\", np.nan\n",
    "        # Handling for any URLs that don't have the correct protocol\n",
    "        except MissingSchema:\n",
    "            return \"\", np.nan\n",
    "\n",
    "\n",
    "def process_page(response):\n",
    "    all_relative = re.findall('href=(\\'|\")(.*?)(\\'|\")', response.content.decode('utf-8'))\n",
    "    for aa in all_relative:\n",
    "        if 'http' in aa[1]:\n",
    "            continue\n",
    "        push_url('http://fsb.ru/' + aa[1])\n",
    "    all_abs = re.findall('href=(\\'|\")(http.*?)(\\'|\")', response.content.decode('utf-8'))\n",
    "    for aa in all_relative:\n",
    "        if not 'http' in aa[1]:\n",
    "            continue\n",
    "        push_url(aa[1])\n",
    "\n",
    "\n",
    "urls = []\n",
    "done = []\n",
    "done_urls = []\n",
    "\n",
    "\n",
    "def push_url(url):\n",
    "    global urls, done_urls\n",
    "    if url not in done_urls:\n",
    "        print(f'get {url} ... [{len(done)}]', end='')\n",
    "\n",
    "        try:\n",
    "            page, text = parse_page(urllib.parse.quote(url))\n",
    "        except Exception as e:\n",
    "            print(f'exception: {e}')\n",
    "        else:\n",
    "            print(f'{len(page)}')\n",
    "            time.sleep(1)\n",
    "            done_urls.append(url)\n",
    "            done.append({'url': url, 'page': page, 'text': text})\n",
    "\n",
    "\n",
    "single_url = 'http://www.fsb.ru/'\n",
    "page, text = parse_page(single_url)\n",
    "print(urls)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-01T15:00:08.730759900Z",
     "start_time": "2023-10-01T15:00:08.489768100Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
